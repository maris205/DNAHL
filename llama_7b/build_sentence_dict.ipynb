{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e0af400-e9ef-46d9-b9b4-b6f6acf66252",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: http://mirrors.aliyun.com/pypi/simple\n",
      "Requirement already satisfied: sentencepiece in /root/miniconda3/lib/python3.12/site-packages (0.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f44e5b49-ae31-4d3d-8d20-1a65b39fa7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: sampled_data.txt\n",
      "  input_format: \n",
      "  model_prefix: dna_llama_seg\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 30000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(353) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(185) LOG(INFO) Loading corpus: sampled_data.txt\n",
      "trainer_interface.cc(147) LOG(INFO) Loaded 1000000 lines\n",
      "trainer_interface.cc(124) LOG(WARNING) Too many sentences are loaded! (1079595), which may slow down training.\n",
      "trainer_interface.cc(126) LOG(WARNING) Consider using --input_sentence_size=<size> and --shuffle_input_sentence=true.\n",
      "trainer_interface.cc(129) LOG(WARNING) They allow to randomly sample <size> sentences from the entire corpus.\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 1079595 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=1080669660\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=5\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 1079595 sentences.\n",
      "unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=733886843\n",
      "unigram_model_trainer.cc(312) LOG(INFO) Initialized 1000005 seed sentencepieces\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 1079595\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 1079592\n",
      "unigram_model_trainer.cc(602) LOG(INFO) Using 1079592 sentences for EM training\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=1000005 obj=1327.41 num_tokens=111402549 num_tokens/piece=111.402\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=999597 obj=1320.99 num_tokens=111741504 num_tokens/piece=111.787\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=749697 obj=1324.65 num_tokens=113626877 num_tokens/piece=151.564\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=749696 obj=1318.12 num_tokens=113895853 num_tokens/piece=151.923\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=562272 obj=1324.55 num_tokens=116257345 num_tokens/piece=206.764\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=562272 obj=1317.12 num_tokens=116330059 num_tokens/piece=206.893\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=421704 obj=1325.23 num_tokens=118859393 num_tokens/piece=281.855\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=421704 obj=1316.79 num_tokens=118864468 num_tokens/piece=281.867\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=316278 obj=1326.36 num_tokens=121637466 num_tokens/piece=384.59\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=316278 obj=1316.81 num_tokens=121623373 num_tokens/piece=384.546\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=237208 obj=1327.39 num_tokens=124655911 num_tokens/piece=525.513\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=237208 obj=1317.11 num_tokens=124643123 num_tokens/piece=525.459\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=177906 obj=1328.13 num_tokens=127876511 num_tokens/piece=718.787\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=177906 obj=1317.67 num_tokens=127870091 num_tokens/piece=718.751\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=133429 obj=1328.7 num_tokens=131236288 num_tokens/piece=983.566\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=133429 obj=1318.44 num_tokens=131239778 num_tokens/piece=983.593\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=100071 obj=1329.47 num_tokens=134612826 num_tokens/piece=1345.17\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=100071 obj=1319.5 num_tokens=134626918 num_tokens/piece=1345.31\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=75053 obj=1330.68 num_tokens=138023032 num_tokens/piece=1839.01\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=75053 obj=1320.87 num_tokens=138046240 num_tokens/piece=1839.32\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=56289 obj=1333.18 num_tokens=141579916 num_tokens/piece=2515.23\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=56289 obj=1323.17 num_tokens=141613358 num_tokens/piece=2515.83\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=42216 obj=1336.5 num_tokens=145332857 num_tokens/piece=3442.6\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=42216 obj=1326.05 num_tokens=145368238 num_tokens/piece=3443.44\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=33000 obj=1337.67 num_tokens=148757801 num_tokens/piece=4507.81\n",
      "unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=33000 obj=1328.18 num_tokens=148804120 num_tokens/piece=4509.22\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: dna_llama_seg.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: dna_llama_seg.vocab\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "# 假设我们有一个文本文件 'data.txt'，其中包含我们希望训练的文本数据\n",
    "spm.SentencePieceTrainer.train(input='sampled_data.txt', model_prefix='dna_llama_seg', vocab_size=30000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb8592e-92c6-4c59-8139-ed618f3df7da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'TCGACG', 'GCACGC', 'GACAG', 'CAGCGAG', 'CCCCG', 'CGCACC', 'CGAGCGCG', 'AATGCA', 'CCTGCTC', 'ATA', 'CGCCCG', 'CGCTCGCG', 'CCACCTCC', 'GTAGGC', 'GACAG', 'CGACGCCG', 'CCACCTCC', 'GGCA']\n"
     ]
    }
   ],
   "source": [
    "from sentencepiece import SentencePieceProcessor\n",
    "model_path = \"dna_llama_seg.model\"\n",
    "sp_model = SentencePieceProcessor(model_file=model_path)\n",
    "mm = sp_model.EncodeAsPieces(\"TCGACGGCACGCGACAGCAGCGAGCCCCGCGCACCCGAGCGCGAATGCACCTGCTCATACGCCCGCGCTCGCGCCACCTCCGTAGGCGACAGCGACGCCGCCACCTCCGGCA\")\n",
    "print(mm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98a28fb-fe5b-41d4-b124-69a271c26088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d8bc23c-eb95-45bc-9bfa-8b67b0800ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://hf-mirror.com\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 设置环境变量\n",
    "os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'\n",
    "\n",
    "# 打印环境变量以确认设置成功\n",
    "print(os.environ.get('HF_ENDPOINT'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89e2d33a-6d84-4ef3-b44e-daa57ac81e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 00:23:18.973146: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-25 00:23:18.989784: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-25 00:23:19.006879: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-25 00:23:19.011922: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-25 00:23:19.026187: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-25 00:23:19.879209: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig,AutoModel\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import  AutoConfig, AutoModelForCausalLM,LlamaForCausalLM,LlamaTokenizer\n",
    "from tokenizers import Tokenizer\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68fc5c44-b444-402e-aaf2-0ba4e2000e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 198390\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dna_ft_dataset = load_dataset(\"dnagpt/dna_multi_task_finetune\")\n",
    "dna_ft_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ab4fd3e-5b59-470e-9b46-f0ffd7b9d1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 178551\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 19839\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = dna_ft_dataset[\"train\"].train_test_split(train_size=0.9, seed=42)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ca97f5-6864-4d6f-944a-182ed1fa2f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"dnagpt/dnahlm-llama-7b-sft-v0\") #dnagpt/\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e904c0b2-bf21-4036-b510-8e57177c1767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f4411f136da4fbfbc995d991b82e10d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(61958, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=61958, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LlamaForCausalLM.from_pretrained(\"dnagpt/dnahlm-llama-7b-sft-v0\") #continue pretrain\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b361c5c-c43f-4ed9-a5c7-c72403cd7a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建提示词\n",
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. \"\n",
    "        f\"Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "\n",
    "    input_text = f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "\n",
    "    return instruction_text + input_text + \"\\n\\n### Response:\\n\"\n",
    "\n",
    "#构建提示词\n",
    "def build_prompt(entry):\n",
    "\n",
    "    input_data = format_input(entry)\n",
    "\n",
    "    desired_response = entry['output']\n",
    "\n",
    "    return input_data + desired_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed031a26-d79e-4f50-85d1-169ebd409c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Determine promoter  detection of following dna sequence, The result will be one of the following: Non-promoter, promoter.',\n",
       " 'input': 'GGCCCCCGCCGATGCCGGCCATGGTGGAGAAGGGCCCCGAGGTCTCAGGGAAGCGGAGAGGGAGGAACAACGCGGCCGCCTCCGCCTCCGCCGCCGCCGCCTCCGCCGCCGCCTCGGCCGCCTGCGCCTCGCCAGCCGCCACTGCCGCCTCGGGCGCCGCCGCCTCCTCAGCCTCGGCCGCCGCCGCCTCAGCCGCCGCCGCCCCCAATAATGGCCAGAATAAAAGTTTGGCGGCGGCGGCGCCCAATGGCAACAGCAGCAGCAACTCCTGGGAGGAAGGCAGCTCGGGCTCGTCCAGCG',\n",
       " 'output': 'Non-promoter'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = data[\"test\"][0]\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31bd4bb5-86a6-4046-b510-492b0548323b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Determine promoter  detection of following dna sequence, The result will be one of the following: Non-promoter, promoter.\n",
      "\n",
      "### Input:\n",
      "GGCCCCCGCCGATGCCGGCCATGGTGGAGAAGGGCCCCGAGGTCTCAGGGAAGCGGAGAGGGAGGAACAACGCGGCCGCCTCCGCCTCCGCCGCCGCCGCCTCCGCCGCCGCCTCGGCCGCCTGCGCCTCGCCAGCCGCCACTGCCGCCTCGGGCGCCGCCGCCTCCTCAGCCTCGGCCGCCGCCGCCTCAGCCGCCGCCGCCCCCAATAATGGCCAGAATAAAAGTTTGGCGGCGGCGGCGCCCAATGGCAACAGCAGCAGCAACTCCTGGGAGGAAGGCAGCTCGGGCTCGTCCAGCG\n",
      "\n",
      "### Response:\n",
      "Non-promoter\n"
     ]
    }
   ],
   "source": [
    "prompt = build_prompt(example)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0449aee-1ac6-4db5-873f-afdfb0fc9691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(text, model, tokenizer, max_input_tokens=1000, max_output_tokens=1000):\n",
    "  # Tokenize\n",
    "  input_ids = tokenizer.encode(\n",
    "          text,\n",
    "          return_tensors=\"pt\",\n",
    "          truncation=True,\n",
    "          max_length=max_input_tokens\n",
    "          # return_attention_mask=True,\n",
    "  )\n",
    "\n",
    "  # Generate\n",
    "  device = model.device\n",
    "  generated_tokens_with_prompt = model.generate(\n",
    "    input_ids=input_ids.to(device),\n",
    "    #max_length=max_output_tokens,\n",
    "    max_new_tokens=8,\n",
    "  )\n",
    "\n",
    "  # Decode\n",
    "  generated_text_with_prompt = tokenizer.decode(generated_tokens_with_prompt[0], skip_special_tokens=True)\n",
    "  generated_text_answer = generated_text_with_prompt[len(text):]\n",
    "\n",
    "\n",
    "  return generated_text_answer\n",
    "\n",
    "# 如果需要进一步清理\n",
    "def clean_generated_text(text):\n",
    "    # 去除 'Ġ' 符号并替换为空格\n",
    "    text = text.replace('Ġ', ' ')\n",
    "    # 去除多余的空格\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9041426-eb59-4314-82dd-7b6d6d477783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (test): Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Determine promoter  detection of following dna sequence, The result will be one of the following: Non-promoter, promoter.\n",
      "\n",
      "### Input:\n",
      "GGCCCCCGCCGATGCCGGCCATGGTGGAGAAGGGCCCCGAGGTCTCAGGGAAGCGGAGAGGGAGGAACAACGCGGCCGCCTCCGCCTCCGCCGCCGCCGCCTCCGCCGCCGCCTCGGCCGCCTGCGCCTCGCCAGCCGCCACTGCCGCCTCGGGCGCCGCCGCCTCCTCAGCCTCGGCCGCCGCCGCCTCAGCCGCCGCCGCCCCCAATAATGGCCAGAATAAAAGTTTGGCGGCGGCGGCGCCCAATGGCAACAGCAGCAGCAACTCCTGGGAGGAAGGCAGCTCGGGCTCGTCCAGCG\n",
      "\n",
      "### Response:\n",
      "\n",
      "real answer: Non-promoter\n",
      "--------------------------\n",
      "\n",
      "model's answer: \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "promoter\n"
     ]
    }
   ],
   "source": [
    "input_text = format_input(data[\"test\"][0])\n",
    "\n",
    "print(\"input (test):\", input_text)\n",
    "\n",
    "print(\"real answer:\", data[\"test\"][0][\"output\"])\n",
    "\n",
    "print(\"--------------------------\\n\")\n",
    "\n",
    "print(\"model's answer: \\n\")\n",
    "print(inference(input_text, model, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d1489173-84af-4c8e-b66b-0cdbe42c7ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = data[\"test\"].shuffle(seed=42).select(range(1000))\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for entry in test_data:\n",
    "    input_text = format_input(entry)\n",
    "    #print(input_text)\n",
    "    response_text = inference(input_text, model, tokenizer)\n",
    "    #print(response_text)\n",
    "    data = {\n",
    "        \"instruction\":entry[\"instruction\"],\n",
    "         \"input\":entry[\"input\"],\n",
    "         \"output\":entry[\"output\"],\n",
    "        \"model_response\":response_text\n",
    "    }\n",
    "\n",
    "    data_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39275fe6-ac3b-4558-9f4c-2853a41d48c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 定义输出文件路径\n",
    "output_file = 'gpt2-small2-sft3.json'\n",
    "\n",
    "# 将 Dataset 对象导出为 JSON 文件\n",
    "# test_data.to_json(output_file)\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(data_list, file, indent=4)  # \"indent\" for pretty-printing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7ffaba65-a270-4433-b234-932f5e288f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'▁prom oter'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(tokenizer.tokenize(\"promoter\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7e373a4-6857-4874-b2da-58da2928925d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "presicion 0.3944223107569721 same 0.3944223107569721\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "with open(output_file, \"r\") as file:\n",
    "    test_data = json.load(file)\n",
    "\n",
    "all_num = len(test_data)\n",
    "right_sum = 0\n",
    "same_sum = 0\n",
    "for item in test_data:\n",
    "    output = item[\"output\"]\n",
    "    #output = \" \".join(tokenizer.tokenize(output))\n",
    "    model_response = item[\"model_response\"]\n",
    "    if model_response == output: #same it\n",
    "        same_sum = same_sum + 1\n",
    "        \n",
    "    if output.find(\"Non\")==-1: # no Non\n",
    "        if model_response.find(output)!=-1 and model_response.find(\"Non\")==-1: #find it, but no Non\n",
    "            right_sum = right_sum + 1\n",
    "    else:\n",
    "        if model_response.find(output)!=-1: #find it\n",
    "            right_sum = right_sum + 1\n",
    "\n",
    "\n",
    "print(\"presicion\", right_sum/all_num, \"same\", same_sum/all_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "446c1cfb-3e26-427a-bc62-17d16198b839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965c101b7c5d403891ed5c63795e26fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'The key to life is to be able to be in the moment and to be able to enjoy it.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"dnahlm-llama-7b-sft-v0\"\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=model_id, \n",
    "    torch_dtype=torch.bfloat16, \n",
    "    max_length=1000,\n",
    "    device_map=\"auto\",\n",
    "    token=\"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\"\n",
    ")\n",
    "\n",
    "pipe(\"The key to life is\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2383bb75-2379-49d0-aa20-9f63de4d39b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDetermine promoter  detection of following dna sequence, The result will be one of the following: Non-promoter, promoter.\\n\\n### Input:\\nGGCCCCCGCCGATGCCGGCCATGGTGGAGAAGGGCCCCGAGGTCTCAGGGAAGCGGAGAGGGAGGAACAACGCGGCCGCCTCCGCCTCCGCCGCCGCCGCCTCCGCCGCCGCCTCGGCCGCCTGCGCCTCGCCAGCCGCCACTGCCGCCTCGGGCGCCGCCGCCTCCTCAGCCTCGGCCGCCGCCGCCTCAGCCGCCGCCGCCCCCAATAATGGCCAGAATAAAAGTTTGGCGGCGGCGGCGCCCAATGGCAACAGCAGCAGCAACTCCTGGGAGGAAGGCAGCTCGGGCTCGTCCAGCG\\n\\n### Response:\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28bbd043-4fd9-402d-bf84-646baaeaaffd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nDetermine promoter  detection of following dna sequence, The result will be one of the following: Non-promoter, promoter.\\n\\n### Input:\\nGGCCCCCGCCGATGCCGGCCATGGTGGAGAAGGGCCCCGAGGTCTCAGGGAAGCGGAGAGGGAGGAACAACGCGGCCGCCTCCGCCTCCGCCGCCGCCGCCTCCGCCGCCGCCTCGGCCGCCTGCGCCTCGCCAGCCGCCACTGCCGCCTCGGGCGCCGCCGCCTCCTCAGCCTCGGCCGCCGCCGCCTCAGCCGCCGCCGCCCCCAATAATGGCCAGAATAAAAGTTTGGCGGCGGCGGCGCCCAATGGCAACAGCAGCAGCAACTCCTGGGAGGAAGGCAGCTCGGGCTCGTCCAGCG\\n\\n### Response:\\npromoter'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294d46f3-2f5b-4e55-ae41-081d5195f5e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
